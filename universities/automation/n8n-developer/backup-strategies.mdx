---
title: Backup, Recovery & Maintenance Strategies
description: Implement robust backup and disaster recovery for production n8n deployments
icon: shield-check
---

# Backup, Recovery & Maintenance Strategies

Protect your workflows and ensure business continuity with comprehensive backup strategies.

## Backup Architecture Overview

```mermaid
graph TB
    A[n8n Instance] --> B[Backup Components]
    B --> C[Database Backups]
    B --> D[Workflow Exports]
    B --> E[Credentials Backup]
    B --> F[Static Files]
    B --> G[Configuration]

    C --> H[PostgreSQL/MySQL Dumps]
    D --> I[JSON Exports]
    E --> J[Encrypted Storage]
    F --> K[File System Sync]
    G --> L[Environment Variables]
```

## Database Backup Strategies

### PostgreSQL Automated Backups

```bash
#!/bin/bash
# backup-postgres.sh

# Configuration
DB_HOST="localhost"
DB_PORT="5432"
DB_NAME="n8n"
DB_USER="n8n"
BACKUP_DIR="/backups/postgres"
RETENTION_DAYS=30
S3_BUCKET="my-n8n-backups"

# Create backup filename with timestamp
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="n8n_backup_${TIMESTAMP}.sql.gz"

# Ensure backup directory exists
mkdir -p ${BACKUP_DIR}

# Perform backup
echo "Starting PostgreSQL backup..."
PGPASSWORD="${DB_PASSWORD}" pg_dump \
  -h ${DB_HOST} \
  -p ${DB_PORT} \
  -U ${DB_USER} \
  -d ${DB_NAME} \
  --verbose \
  --no-owner \
  --no-acl \
  --clean \
  --if-exists | gzip > ${BACKUP_DIR}/${BACKUP_FILE}

# Check if backup was successful
if [ $? -eq 0 ]; then
  echo "Backup completed: ${BACKUP_FILE}"

  # Upload to S3
  aws s3 cp ${BACKUP_DIR}/${BACKUP_FILE} s3://${S3_BUCKET}/postgres/

  # Clean up old local backups
  find ${BACKUP_DIR} -name "n8n_backup_*.sql.gz" -mtime +${RETENTION_DAYS} -delete

  # Send success notification
  curl -X POST https://hooks.slack.com/services/YOUR/WEBHOOK/URL \
    -H 'Content-Type: application/json' \
    -d "{\"text\":\"✅ n8n backup successful: ${BACKUP_FILE}\"}"
else
  echo "Backup failed!"
  # Send failure notification
  curl -X POST https://hooks.slack.com/services/YOUR/WEBHOOK/URL \
    -H 'Content-Type: application/json' \
    -d '{"text":"❌ n8n backup failed!"}'
  exit 1
fi
```

### MySQL Backup Script

```bash
#!/bin/bash
# backup-mysql.sh

# Configuration
MYSQL_HOST="localhost"
MYSQL_PORT="3306"
MYSQL_DATABASE="n8n"
MYSQL_USER="n8n"
BACKUP_DIR="/backups/mysql"

# Create backup
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="${BACKUP_DIR}/n8n_mysql_${TIMESTAMP}.sql.gz"

# Perform backup with mysqldump
mysqldump \
  --host=${MYSQL_HOST} \
  --port=${MYSQL_PORT} \
  --user=${MYSQL_USER} \
  --password=${MYSQL_PASSWORD} \
  --single-transaction \
  --routines \
  --triggers \
  --events \
  --databases ${MYSQL_DATABASE} | gzip > ${BACKUP_FILE}

echo "MySQL backup completed: ${BACKUP_FILE}"
```

### Continuous Backup with WAL Archiving (PostgreSQL)

```yaml
# docker-compose.yml with WAL archiving
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: n8n
      POSTGRES_USER: n8n
      POSTGRES_PASSWORD: n8n
      POSTGRES_INITDB_ARGS: "-X /var/lib/postgresql/wal"
    command:
      - postgres
      - -c
      - wal_level=replica
      - -c
      - archive_mode=on
      - -c
      - archive_command='test ! -f /archives/%f && cp %p /archives/%f'
      - -c
      - max_wal_senders=3
      - -c
      - wal_keep_size=1GB
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - postgres-wal:/var/lib/postgresql/wal
      - ./archives:/archives

volumes:
  postgres-data:
  postgres-wal:
```

## Workflow Backup & Export

### Automated Workflow Export

```typescript
// workflow-backup.ts
import axios from 'axios';
import fs from 'fs';
import path from 'path';

interface WorkflowBackupConfig {
  n8nUrl: string;
  apiKey: string;
  backupDir: string;
  includeCredentials: boolean;
}

class WorkflowBackup {
  private config: WorkflowBackupConfig;

  constructor(config: WorkflowBackupConfig) {
    this.config = config;
  }

  async backupAllWorkflows(): Promise<void> {
    try {
      // Get all workflows
      const workflows = await this.getAllWorkflows();

      // Create timestamp directory
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
      const backupPath = path.join(this.config.backupDir, `backup-${timestamp}`);
      fs.mkdirSync(backupPath, { recursive: true });

      // Export each workflow
      for (const workflow of workflows) {
        const workflowData = await this.getWorkflow(workflow.id);

        // Save workflow to file
        const filename = `${workflow.id}-${workflow.name.replace(/[^a-z0-9]/gi, '_')}.json`;
        const filepath = path.join(backupPath, filename);

        fs.writeFileSync(filepath, JSON.stringify(workflowData, null, 2));
        console.log(`Backed up workflow: ${workflow.name}`);
      }

      // Create manifest file
      const manifest = {
        timestamp,
        workflowCount: workflows.length,
        workflows: workflows.map(w => ({
          id: w.id,
          name: w.name,
          active: w.active
        }))
      };

      fs.writeFileSync(
        path.join(backupPath, 'manifest.json'),
        JSON.stringify(manifest, null, 2)
      );

      console.log(`Backup completed: ${workflows.length} workflows backed up to ${backupPath}`);
    } catch (error) {
      console.error('Backup failed:', error);
      throw error;
    }
  }

  private async getAllWorkflows(): Promise<any[]> {
    const response = await axios.get(`${this.config.n8nUrl}/api/v1/workflows`, {
      headers: {
        'X-N8N-API-KEY': this.config.apiKey
      }
    });
    return response.data.data;
  }

  private async getWorkflow(id: string): Promise<any> {
    const response = await axios.get(`${this.config.n8nUrl}/api/v1/workflows/${id}`, {
      headers: {
        'X-N8N-API-KEY': this.config.apiKey
      }
    });
    return response.data.data;
  }

  async restoreWorkflows(backupPath: string): Promise<void> {
    // Read manifest
    const manifest = JSON.parse(
      fs.readFileSync(path.join(backupPath, 'manifest.json'), 'utf8')
    );

    console.log(`Restoring ${manifest.workflowCount} workflows from ${manifest.timestamp}`);

    // Restore each workflow
    for (const workflowInfo of manifest.workflows) {
      const filename = `${workflowInfo.id}-${workflowInfo.name.replace(/[^a-z0-9]/gi, '_')}.json`;
      const filepath = path.join(backupPath, filename);

      const workflowData = JSON.parse(fs.readFileSync(filepath, 'utf8'));

      try {
        await this.importWorkflow(workflowData);
        console.log(`Restored workflow: ${workflowInfo.name}`);
      } catch (error) {
        console.error(`Failed to restore ${workflowInfo.name}:`, error);
      }
    }
  }

  private async importWorkflow(workflowData: any): Promise<void> {
    await axios.post(`${this.config.n8nUrl}/api/v1/workflows`, workflowData, {
      headers: {
        'X-N8N-API-KEY': this.config.apiKey
      }
    });
  }
}

// Usage
const backup = new WorkflowBackup({
  n8nUrl: 'http://localhost:5678',
  apiKey: process.env.N8N_API_KEY!,
  backupDir: '/backups/workflows',
  includeCredentials: false
});

// Run backup
backup.backupAllWorkflows();
```

### n8n CLI Backup

```bash
#!/bin/bash
# n8n-cli-backup.sh

# Export all workflows
n8n export:workflow --all --output=/backups/workflows-$(date +%Y%m%d).json

# Export credentials (encrypted)
n8n export:credentials --all --output=/backups/credentials-$(date +%Y%m%d).json

# Backup static files
tar -czf /backups/n8n-files-$(date +%Y%m%d).tar.gz ~/.n8n/

echo "n8n CLI backup completed"
```

## Incremental Backup Strategy

### Delta Sync Implementation

```typescript
// incremental-backup.ts
import crypto from 'crypto';

class IncrementalBackup {
  private lastBackupState: Map<string, string> = new Map();

  async performIncrementalBackup(): Promise<void> {
    const currentWorkflows = await this.getAllWorkflows();
    const changes = {
      added: [],
      modified: [],
      deleted: []
    };

    // Check for new or modified workflows
    for (const workflow of currentWorkflows) {
      const currentHash = this.hashWorkflow(workflow);
      const lastHash = this.lastBackupState.get(workflow.id);

      if (!lastHash) {
        changes.added.push(workflow);
      } else if (currentHash !== lastHash) {
        changes.modified.push(workflow);
      }

      this.lastBackupState.set(workflow.id, currentHash);
    }

    // Check for deleted workflows
    for (const [id, hash] of this.lastBackupState.entries()) {
      if (!currentWorkflows.find(w => w.id === id)) {
        changes.deleted.push(id);
      }
    }

    // Backup only changed items
    await this.backupChanges(changes);
  }

  private hashWorkflow(workflow: any): string {
    const hash = crypto.createHash('sha256');
    hash.update(JSON.stringify(workflow));
    return hash.digest('hex');
  }

  private async backupChanges(changes: any): Promise<void> {
    const timestamp = new Date().toISOString();

    // Create incremental backup file
    const backup = {
      timestamp,
      type: 'incremental',
      changes
    };

    // Save to incremental backup location
    fs.writeFileSync(
      `/backups/incremental/backup-${timestamp}.json`,
      JSON.stringify(backup, null, 2)
    );
  }
}
```

## Disaster Recovery

### Recovery Point Objective (RPO) & Recovery Time Objective (RTO)

```yaml
# disaster-recovery-config.yml
disaster_recovery:
  rpo: 1h  # Maximum data loss tolerance
  rto: 2h  # Maximum downtime tolerance

  backup_schedule:
    full: "0 2 * * 0"      # Weekly full backup
    incremental: "0 */4 * * *"  # Every 4 hours
    transaction_log: "*/15 * * * *"  # Every 15 minutes

  recovery_tiers:
    tier1_critical:
      - authentication_workflows
      - payment_processing
      - customer_notifications

    tier2_important:
      - reporting_workflows
      - data_sync_workflows

    tier3_standard:
      - internal_automations
      - maintenance_tasks
```

### Automated Recovery Script

```bash
#!/bin/bash
# disaster-recovery.sh

set -e

# Configuration
BACKUP_SOURCE="s3://n8n-backups"
RESTORE_DIR="/tmp/n8n-restore"
N8N_URL="http://localhost:5678"
POSTGRES_HOST="localhost"
POSTGRES_DB="n8n"

echo "Starting n8n Disaster Recovery Process..."

# Step 1: Stop n8n
echo "Stopping n8n service..."
docker-compose stop n8n

# Step 2: Restore database
echo "Restoring database from latest backup..."
LATEST_DB_BACKUP=$(aws s3 ls ${BACKUP_SOURCE}/postgres/ | sort | tail -n 1 | awk '{print $4}')
aws s3 cp ${BACKUP_SOURCE}/postgres/${LATEST_DB_BACKUP} ${RESTORE_DIR}/

# Drop existing database and restore
psql -h ${POSTGRES_HOST} -U postgres -c "DROP DATABASE IF EXISTS ${POSTGRES_DB};"
psql -h ${POSTGRES_HOST} -U postgres -c "CREATE DATABASE ${POSTGRES_DB};"
gunzip -c ${RESTORE_DIR}/${LATEST_DB_BACKUP} | psql -h ${POSTGRES_HOST} -U postgres ${POSTGRES_DB}

# Step 3: Restore file system
echo "Restoring file system..."
LATEST_FILES_BACKUP=$(aws s3 ls ${BACKUP_SOURCE}/files/ | sort | tail -n 1 | awk '{print $4}')
aws s3 cp ${BACKUP_SOURCE}/files/${LATEST_FILES_BACKUP} ${RESTORE_DIR}/
tar -xzf ${RESTORE_DIR}/${LATEST_FILES_BACKUP} -C /

# Step 4: Restart n8n
echo "Starting n8n service..."
docker-compose up -d n8n

# Step 5: Verify recovery
echo "Verifying recovery..."
sleep 30

# Check if n8n is responsive
if curl -f ${N8N_URL}/healthz > /dev/null 2>&1; then
  echo "✅ Recovery successful! n8n is running."

  # Test workflow execution
  TEST_RESULT=$(curl -X POST ${N8N_URL}/webhook-test/health-check 2>/dev/null)
  if [ "$TEST_RESULT" = "OK" ]; then
    echo "✅ Workflow execution verified!"
  else
    echo "⚠️ Warning: n8n is running but workflow execution needs verification"
  fi
else
  echo "❌ Recovery failed! n8n is not responding"
  exit 1
fi

echo "Disaster recovery completed at $(date)"
```

## Monitoring & Health Checks

### Health Check Workflow

```typescript
// health-check.node.ts
export class HealthCheckNode implements INodeType {
  description: INodeTypeDescription = {
    displayName: 'Health Check',
    name: 'healthCheck',
    group: ['utility'],
    version: 1,
    description: 'Monitor n8n instance health',
    inputs: ['main'],
    outputs: ['main'],
    properties: []
  };

  async execute(this: IExecuteFunctions): Promise<INodeExecutionData[][]> {
    const healthMetrics = {
      timestamp: new Date().toISOString(),
      status: 'healthy',
      checks: {
        database: await this.checkDatabase(),
        filesystem: await this.checkFilesystem(),
        memory: await this.checkMemory(),
        workflows: await this.checkWorkflows()
      }
    };

    // Send to monitoring system
    await this.helpers.request({
      method: 'POST',
      uri: 'http://monitoring.example.com/health',
      body: healthMetrics,
      json: true
    });

    return [[{ json: healthMetrics }]];
  }

  private async checkDatabase(): Promise<any> {
    try {
      const result = await this.helpers.request({
        uri: 'http://localhost:5678/api/v1/workflows?limit=1'
      });
      return { status: 'ok', responseTime: Date.now() };
    } catch (error) {
      return { status: 'error', error: error.message };
    }
  }

  private async checkFilesystem(): Promise<any> {
    const fs = require('fs').promises;
    const stats = await fs.statfs('/');

    return {
      status: 'ok',
      freeSpace: stats.bfree * stats.bsize,
      totalSpace: stats.blocks * stats.bsize,
      usagePercent: ((stats.blocks - stats.bfree) / stats.blocks) * 100
    };
  }

  private async checkMemory(): Promise<any> {
    const used = process.memoryUsage();

    return {
      status: 'ok',
      heapUsed: used.heapUsed,
      heapTotal: used.heapTotal,
      rss: used.rss,
      external: used.external
    };
  }

  private async checkWorkflows(): Promise<any> {
    // Check critical workflows
    const criticalWorkflows = ['payment-processor', 'user-sync', 'notification-sender'];
    const results = {};

    for (const workflowName of criticalWorkflows) {
      // Check if workflow exists and is active
      results[workflowName] = 'active'; // Simplified
    }

    return results;
  }
}
```

### Monitoring Dashboard

```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
    ports:
      - "3000:3000"

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://n8n:n8n@postgres:5432/n8n?sslmode=disable"
    ports:
      - "9187:9187"

  node-exporter:
    image: prom/node-exporter
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)'
    ports:
      - "9100:9100"

volumes:
  prometheus-data:
  grafana-data:
```

## Maintenance Procedures

### Rolling Updates

```bash
#!/bin/bash
# rolling-update.sh

# Configuration
INSTANCES=("n8n-1" "n8n-2" "n8n-3")
NEW_VERSION="1.20.0"

for instance in "${INSTANCES[@]}"; do
  echo "Updating ${instance} to version ${NEW_VERSION}..."

  # Drain connections
  docker exec ${instance} n8n healthcheck --drain

  # Wait for active executions to complete
  sleep 30

  # Update instance
  docker stop ${instance}
  docker rm ${instance}
  docker run -d --name ${instance} n8nio/n8n:${NEW_VERSION}

  # Health check
  for i in {1..30}; do
    if docker exec ${instance} n8n healthcheck; then
      echo "${instance} updated successfully"
      break
    fi
    sleep 10
  done

  # Wait before updating next instance
  sleep 60
done
```

### Database Maintenance

```sql
-- maintenance.sql
-- Vacuum and analyze tables
VACUUM ANALYZE workflow_entity;
VACUUM ANALYZE execution_entity;
VACUUM ANALYZE webhook_entity;

-- Reindex for performance
REINDEX TABLE workflow_entity;
REINDEX TABLE execution_entity;

-- Clean old executions
DELETE FROM execution_entity
WHERE finished_at < NOW() - INTERVAL '30 days'
AND stored_data = true;

-- Update statistics
ANALYZE;
```

## Security Best Practices

### Credential Encryption

```typescript
// credential-encryption.ts
import crypto from 'crypto';

class CredentialEncryption {
  private algorithm = 'aes-256-gcm';
  private key: Buffer;

  constructor(masterKey: string) {
    this.key = crypto.scryptSync(masterKey, 'salt', 32);
  }

  encrypt(credentials: any): string {
    const iv = crypto.randomBytes(16);
    const cipher = crypto.createCipheriv(this.algorithm, this.key, iv);

    let encrypted = cipher.update(JSON.stringify(credentials), 'utf8', 'hex');
    encrypted += cipher.final('hex');

    const authTag = cipher.getAuthTag();

    return iv.toString('hex') + ':' + authTag.toString('hex') + ':' + encrypted;
  }

  decrypt(encryptedData: string): any {
    const parts = encryptedData.split(':');
    const iv = Buffer.from(parts[0], 'hex');
    const authTag = Buffer.from(parts[1], 'hex');
    const encrypted = parts[2];

    const decipher = crypto.createDecipheriv(this.algorithm, this.key, iv);
    decipher.setAuthTag(authTag);

    let decrypted = decipher.update(encrypted, 'hex', 'utf8');
    decrypted += decipher.final('utf8');

    return JSON.parse(decrypted);
  }
}
```

## Backup Validation

```bash
#!/bin/bash
# validate-backup.sh

BACKUP_FILE=$1
TEMP_DB="n8n_restore_test"

echo "Validating backup: ${BACKUP_FILE}"

# Create temporary database
psql -U postgres -c "CREATE DATABASE ${TEMP_DB};"

# Restore backup to temporary database
gunzip -c ${BACKUP_FILE} | psql -U postgres ${TEMP_DB}

# Run validation queries
WORKFLOW_COUNT=$(psql -U postgres -d ${TEMP_DB} -t -c "SELECT COUNT(*) FROM workflow_entity;")
EXECUTION_COUNT=$(psql -U postgres -d ${TEMP_DB} -t -c "SELECT COUNT(*) FROM execution_entity;")

echo "Workflows: ${WORKFLOW_COUNT}"
echo "Executions: ${EXECUTION_COUNT}"

# Cleanup
psql -U postgres -c "DROP DATABASE ${TEMP_DB};"

if [ ${WORKFLOW_COUNT} -gt 0 ]; then
  echo "✅ Backup validation successful"
  exit 0
else
  echo "❌ Backup validation failed"
  exit 1
fi
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Production Deployment" href="./production-deployment" icon="rocket">
    Deploy n8n at scale
  </Card>
  <Card title="API Development" href="./api-integration" icon="code">
    Build on n8n's API
  </Card>
</CardGroup>